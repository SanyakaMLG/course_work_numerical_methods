{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f6a2a-d0b1-4a28-b054-90e820b6b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c0cc6e-8810-4231-8c80-cc38e13a9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('breast-cancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab262a65-51cc-46f1-aeea-6dbf287e4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728d77d-f8a2-4053-8993-f15ad042bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['id'], errors='ignore')\n",
    "\n",
    "df['diagnosis'] = df['diagnosis'].map({'B': 0, 'M': 1})\n",
    "\n",
    "X = df.drop(columns=['diagnosis']).values\n",
    "y = df['diagnosis'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f9953-e7c3-43bd-9844-abf56703233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_dict = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"tanh\": nn.Tanh,\n",
    "    \"sigmoid\": nn.Sigmoid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a31c8-9f3f-4541-be5a-412f070cf3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySGD(Optimizer):\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=0.01):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Некорректное значение lr: {lr}\")\n",
    "\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "\n",
    "                p.data.add_(grad, alpha=-lr)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6522fae7-829e-4a8c-a35b-90ad5139457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNAG(Optimizer):\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=0.01,\n",
    "                 momentum=0.9):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Некорректное значение lr: {lr}\")\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(f\"Некорректное значение momentum: {momentum}\")\n",
    "\n",
    "        defaults = dict(lr=lr,\n",
    "                        momentum=momentum)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            momentum = group['momentum']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "\n",
    "                param_state = self.state[p]\n",
    "                if 'momentum_buffer' not in param_state:\n",
    "                    buf = param_state['momentum_buffer'] = grad.clone().detach()\n",
    "                else:\n",
    "                    buf = param_state['momentum_buffer']\n",
    "                    buf.mul_(momentum).add_(grad)\n",
    "\n",
    "                nesterov_grad = grad.add(buf, alpha=momentum)\n",
    "                p.data.add_(nesterov_grad, alpha=-lr)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c157d4-78d1-455a-af43-0aba79c0aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAdagrad(Optimizer):\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-2,\n",
    "                 eps=1e-10):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Некорректное значение lr: {lr}\")\n",
    "        if eps < 0.0:\n",
    "            raise ValueError(f\"Некорректное значение eps: {eps}\")\n",
    "\n",
    "        defaults = dict(lr=lr,\n",
    "                        eps=eps,\n",
    "                        step=0)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            eps = group['eps']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "\n",
    "                state = self.state[p]\n",
    "                if 'sum_squares' not in state:\n",
    "                    state['sum_squares'] = torch.zeros_like(p.data)\n",
    "\n",
    "                sum_squares = state['sum_squares']\n",
    "                sum_squares.addcmul_(grad, grad, value=1.0)\n",
    "\n",
    "                p.data.addcdiv_(grad, sum_squares.sqrt().add_(eps), value=-lr)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a0469-4ae6-403f-baeb-1050eb85d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRMSprop(Optimizer):\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-2,\n",
    "                 alpha=0.99,\n",
    "                 eps=1e-8):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Некорректное значение lr: {lr}\")\n",
    "        if alpha < 0.0:\n",
    "            raise ValueError(f\"Некорректное значение alpha: {alpha}\")\n",
    "        if eps < 0.0:\n",
    "            raise ValueError(f\"Некорректное значение eps: {eps}\")\n",
    "\n",
    "        defaults = dict(lr=lr,\n",
    "                        alpha=alpha,\n",
    "                        eps=eps)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            alpha = group['alpha']\n",
    "            eps = group['eps']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if 'square_avg' not in state:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "                square_avg = state['square_avg']\n",
    "\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "\n",
    "                denom = square_avg.sqrt().add_(eps)\n",
    "\n",
    "                p.data.addcdiv_(grad, denom, value=-lr)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e46bc-e6d3-40bb-be6a-8f818b72887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAdam(Optimizer):\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-3,\n",
    "                 betas=(0.9, 0.999),\n",
    "                 eps=1e-8):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Некорректное значение lr: {lr}\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Некорректное значение beta1: {betas[0]}\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Некорректное значение beta2: {betas[1]}\")\n",
    "        if eps < 0.0:\n",
    "            raise ValueError(f\"Некорректное значение eps: {eps}\")\n",
    "\n",
    "        defaults = dict(lr=lr,\n",
    "                        betas=betas,\n",
    "                        eps=eps,\n",
    "                        step=0)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            betas = group['betas']\n",
    "            eps = group['eps']\n",
    "\n",
    "            group['step'] += 1\n",
    "            step_num = group['step']\n",
    "            beta1, beta2 = betas\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if 'exp_avg' not in state:\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                exp_avg = state['exp_avg']\n",
    "\n",
    "                if 'exp_avg_sq' not in state:\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                exp_avg_sq = state['exp_avg_sq']\n",
    "\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(eps)\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** step_num\n",
    "                bias_correction2 = 1 - beta2 ** step_num\n",
    "                step_size = lr * (bias_correction2 ** 0.5) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad45ce1-d124-4a48-8173-04cd8d1daca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim, hidden_config, output_dim=1):\n",
    "    \"\"\"\n",
    "    Создаёт MLP-модель с заданной конфигурацией:\n",
    "      hidden_config: [(n_neurons, activation_name), (n_neurons, activation_name), ...]\n",
    "      output_dim: количество нейронов на выходном слое (у нас 1, т.к. бинарная классификация)\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    in_dim = input_dim\n",
    "\n",
    "    for (n_neurons, act_name) in hidden_config:\n",
    "        layers.append(nn.Linear(in_dim, n_neurons))\n",
    "        layers.append(activation_dict[act_name]())\n",
    "        in_dim = n_neurons\n",
    "\n",
    "    # Выходной слой\n",
    "    layers.append(nn.Linear(in_dim, output_dim))\n",
    "    # Для бинарной классификации (через BCEWithLogitsLoss) активацию Sigmoid не добавляем здесь\n",
    "    model = nn.Sequential(*layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4382f7d8-7517-4a00-bb64-e109c76803e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                X_train, y_train, \n",
    "                X_val, y_val, \n",
    "                epochs=20, \n",
    "                batch_size=32, \n",
    "                lr=1e-3, \n",
    "                device='cpu',\n",
    "                optimizer='Adam'):\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    X_val_t   = torch.tensor(X_val,   dtype=torch.float32).to(device)\n",
    "    y_val_t   = torch.tensor(y_val,   dtype=torch.float32).view(-1, 1).to(device)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    if optimizer == 'Adam':\n",
    "        optimizer = MyAdam(model.parameters(), lr=lr)\n",
    "    elif optimizer == 'NAG':\n",
    "        optimizer = MyNAG(model.parameters(), lr=lr)\n",
    "    elif optimizer == 'RMSProp':\n",
    "        optimizer = MyRMSprop(model.parameters(), lr=lr)\n",
    "    elif optimizer == 'Adagrad':\n",
    "        optimizer = MyAdagrad(model.parameters(), lr=lr)\n",
    "    elif optimizer == 'SGD':\n",
    "        optimizer = MySGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError('Unknow optimizer parameter')\n",
    "        \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        permutation = torch.randperm(X_train_t.size(0))\n",
    "        for i in range(0, X_train_t.size(0), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = X_train_t[indices], y_train_t[indices]\n",
    "\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_t)\n",
    "        val_preds = (torch.sigmoid(val_outputs) >= 0.5).float()\n",
    "        correct = (val_preds == y_val_t).sum().item()\n",
    "        val_acc = correct / len(y_val_t)\n",
    "\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f90545-ca00-454d-a0ab-603cb3ed7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_cv(hidden_config, X, y, n_splits=5, epochs=20, batch_size=32, lr=1e-3, device='cpu', optimizer='Adam', verbose=False):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_accuracies = []\n",
    "    \n",
    "    iterator = enumerate(skf.split(X, y))\n",
    "    \n",
    "    if verbose:\n",
    "        iterator = tqdm(iterator)\n",
    "\n",
    "    for idx, (train_idx, val_idx) in iterator:\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val   = scaler.transform(X_val)\n",
    "\n",
    "        input_dim = X.shape[1]\n",
    "        model = create_model(input_dim, hidden_config)\n",
    "\n",
    "        val_acc = train_model(model, \n",
    "                              X_train, y_train, \n",
    "                              X_val,   y_val, \n",
    "                              epochs=epochs, \n",
    "                              batch_size=batch_size, \n",
    "                              lr=lr, \n",
    "                              device=device,\n",
    "                              optimizer=optimizer)\n",
    "        fold_accuracies.append(val_acc)\n",
    "\n",
    "    return np.mean(fold_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e3f291-6d0e-4500-9995-b2a3f3f36be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_cv(\n",
    "    [(16, \"relu\")],\n",
    "    X,\n",
    "    y,\n",
    "    lr=1e-2,\n",
    "    n_splits=100,\n",
    "    device='cuda',\n",
    "    optimizer='SGD',\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e486ff-e4bd-477c-94b5-030aa0d30924",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_cv(\n",
    "    [(16, \"relu\")],\n",
    "    X,\n",
    "    y,\n",
    "    lr=1e-2,\n",
    "    n_splits=100,\n",
    "    device='cuda',\n",
    "    optimizer='NAG',\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5c4ba-faed-4b21-956f-de8112e0830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_cv(\n",
    "    [(16, \"relu\")],\n",
    "    X,\n",
    "    y,\n",
    "    lr=1e-2,\n",
    "    n_splits=100,\n",
    "    device='cuda',\n",
    "    optimizer='Adagrad',\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c7c13-e82e-4f1e-9090-6ff493ace12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_cv(\n",
    "    [(16, \"relu\")],\n",
    "    X,\n",
    "    y,\n",
    "    lr=1e-2,\n",
    "    n_splits=100,\n",
    "    device='cuda',\n",
    "    optimizer='RMSProp',\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a21ec-9402-41e9-b9c6-33b24f50703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_cv(\n",
    "    [(16, \"relu\")],\n",
    "    X,\n",
    "    y,\n",
    "    lr=1e-2,\n",
    "    n_splits=100,\n",
    "    device='cuda',\n",
    "    optimizer='Adam',\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc35eb-64a2-4c2c-9c8f-3db81b058d7e",
   "metadata": {},
   "source": [
    "# Genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Будем кодировать гиперпараметры так:\n",
    "- Число скрытых слоёв H: 1..3 (например)\n",
    "- Для каждого слоя: количество нейронов N: 4..64 (пример диапазона)\n",
    "- Для каждого слоя: функция активации: ['relu', 'tanh', 'sigmoid']\n",
    "\n",
    "То есть, если H=2, то особь должна хранить:\n",
    "[\n",
    "  (n_neurons_layer1, activation_layer1),\n",
    "  (n_neurons_layer2, activation_layer2)\n",
    "]\n",
    "\n",
    "Если H=1 — только один слой, если H=3 — три слоя и т.д.\n",
    "\n",
    "Для упрощения можно хранить в особи структуру вида:\n",
    "[H, n1, act1, n2, act2, n3, act3]\n",
    "\n",
    "Но придётся аккуратно интерпретировать в функции evaluate.\n",
    "\"\"\"\n",
    "\n",
    "H_min, H_max = 1, 5\n",
    "N_min, N_max = 4, 128\n",
    "activations = list(activation_dict.keys())  # [\"relu\", \"tanh\", \"sigmoid\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea67b6e17e81eccd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0ccb2-34e9-47ff-a166-297f1a672d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Individual:\n",
    "    def __init__(self, genes):\n",
    "        self.genes = genes\n",
    "        self.fitness = 0.0\n",
    "\n",
    "\n",
    "class GeneticOptimizer:\n",
    "    def __init__(self, pop_size=15, n_generations=10, cx_prob=0.5,\n",
    "                 mut_prob=0.2, n_epochs=5):\n",
    "        self.pop_size = pop_size\n",
    "        self.n_generations = n_generations\n",
    "        self.cx_prob = cx_prob\n",
    "        self.mut_prob = mut_prob\n",
    "        self.n_epochs = n_epochs\n",
    "        self.hall_of_fame = None\n",
    "\n",
    "    def _create_individual(self):\n",
    "        genes = [\n",
    "            random.randint(H_min, H_max + 1),  # num layers\n",
    "            random.randint(N_min, N_max + 1),\n",
    "            random.choice(activations),\n",
    "            random.randint(N_min, N_max + 1),\n",
    "            random.choice(activations),\n",
    "            random.randint(N_min, N_max + 1),\n",
    "            random.choice(activations),\n",
    "            random.randint(N_min, N_max + 1),\n",
    "            random.choice(activations),\n",
    "            random.randint(N_min, N_max + 1),\n",
    "            random.choice(activations),\n",
    "        ]\n",
    "        return Individual(genes)\n",
    "\n",
    "    def initialize_population(self):\n",
    "        return [self._create_individual() for _ in range(self.pop_size)]\n",
    "    \n",
    "    def decode_individual(self, ind):\n",
    "        H = ind.genes[0]\n",
    "        hidden_config = []\n",
    "        for i in range(H):\n",
    "            n_i = ind.genes[1 + i*2]\n",
    "            a_i = ind.genes[2 + i*2]\n",
    "            hidden_config.append((n_i, a_i))\n",
    "        return hidden_config\n",
    "\n",
    "    def evaluate_population(self, population, X, y):\n",
    "        for ind in population:\n",
    "            hidden_config = self.decode_individual(ind)\n",
    "\n",
    "            acc = evaluate_model_cv(hidden_config, X, y, \n",
    "                                    n_splits=100,\n",
    "                                    epochs=20, \n",
    "                                    batch_size=32,\n",
    "                                    lr=1e-2,\n",
    "                                    device='cuda')\n",
    "            ind.fitness = acc\n",
    "\n",
    "        # Update hall of fame\n",
    "        current_best = max(population, key=lambda x: x.fitness)\n",
    "        if not self.hall_of_fame or current_best.fitness > self.hall_of_fame.fitness:\n",
    "            self.hall_of_fame = current_best\n",
    "\n",
    "    def _select_parent(self, population):\n",
    "        tournament = random.sample(population, 3)\n",
    "        return max(tournament, key=lambda x: x.fitness)\n",
    "\n",
    "    def _crossover(self, parent1, parent2):\n",
    "        if random.random() > self.cx_prob:\n",
    "            return parent1, parent2\n",
    "        \n",
    "        cxpoint = random.randint(1, len(ind1))\n",
    "        child1 = Individual(parent1.genes[:cxpoint] + parent2.genes[cxpoint])\n",
    "        child2 = Individual(parent2.genes[:cxpoint] + parent1.genes[cxpoint])\n",
    "        return child1, child2\n",
    "\n",
    "    def _mutate(self, individual):\n",
    "        for i in range(len(individual.genes)):\n",
    "            if random.random() < self.mut_prob:\n",
    "                if i == 0:\n",
    "                    individual.genes[i] = random.randint(H_min, H_max+1)\n",
    "                elif i % 2 == 1:\n",
    "                    individual.genes[i] = random.randint(N_min, N_max+1)\n",
    "                else:\n",
    "                    if i > 0:\n",
    "                        individual.genes[i] = np.random.choice(activations)\n",
    "        return individual\n",
    "\n",
    "    def evolve(self, X, y):\n",
    "        population = self.initialize_population()\n",
    "\n",
    "        for gen in range(self.n_generations):\n",
    "            self.evaluate_population(population, X, y)\n",
    "\n",
    "            fitnesses = [ind.fitness for ind in population]\n",
    "            print(f\"\\nGeneration {gen + 1}/{self.n_generations}\")\n",
    "            print(f\"Max Fitness: {max(fitnesses):.2f}\")\n",
    "            print(f\"Avg Fitness: {np.mean(fitnesses):.2f}\")\n",
    "            print(f\"Min Fitness: {min(fitnesses):.2f}\")\n",
    "\n",
    "            new_pop = []\n",
    "            while len(new_pop) < self.pop_size:\n",
    "                parent1 = self._select_parent(population)\n",
    "                parent2 = self._select_parent(population)\n",
    "                child1, child2 = self._crossover(parent1, parent2)\n",
    "\n",
    "                for child in [child1, child2]:\n",
    "                    if len(new_pop) >= self.pop_size:\n",
    "                        break\n",
    "                    self._mutate(child)\n",
    "                    new_pop.append(child)\n",
    "\n",
    "            population = new_pop\n",
    "\n",
    "        return self.hall_of_fame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e31c4a-1923-4c9c-96a2-3e0d9545bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "DEVICES = [0, 1, 2]\n",
    "N_GENERATIONS = 10\n",
    "POP_SIZE = 15\n",
    "\n",
    "ga = GeneticOptimizer(\n",
    "        pop_size=POP_SIZE,\n",
    "        n_generations=N_GENERATIONS,\n",
    "        n_epochs=50\n",
    "    )\n",
    "\n",
    "start = time.monotonic()\n",
    "best = ga.evolve(X, y)\n",
    "end = time.monotonic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c2662-561d-414a-95c5-0aa620249f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
